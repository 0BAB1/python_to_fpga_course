{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71a8839-7c58-4862-a5f0-0f7e97085918",
   "metadata": {},
   "source": [
    "# THIS NOTEBOOK TRAINS A FASHION-MNIST MODEL USING 0-255 VALUES (UINT8)\n",
    "\n",
    "After creating the docker environement,\n",
    "\n",
    "The goal of this lab is to create a MNIST Fasion model in pytorch and experiment with the different parameters\n",
    "\n",
    "Then, we will do the same model but fully quantized and start adapting it for FINN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09c1a7-e82e-4d14-bced-29aaf340638c",
   "metadata": {},
   "source": [
    "## Base model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e9dec1-299c-45ec-bc1a-41d4825fea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root_dir = \"/tmp/finn_dev_rootmin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9651dd1-25d9-4c48-8e02-08c28d29fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    #transforms.Normalize((0.5,), (0.5,))  # Normalize the tensor with mean and std\n",
    "]);\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,  # Load the training set\n",
    "    download=True,  # Download the dataset if it doesn't exist\n",
    "    transform=transform  # Apply the defined transformations\n",
    ");\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb48f47-1369-4f87-8c9b-c0e29a8de5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset[5]\n",
    "image = np.array(image).squeeze()\n",
    "print(\"Min : \", np.min(image[0]), \" /// Max : \", np.max(image[0]))\n",
    "# plot the sample\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdce0f7-c128-48f5-956e-7174818139d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# Create a data loader for the training set\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,  # Number of samples per batch\n",
    "    shuffle=True  # Shuffle the data\n",
    ")\n",
    "\n",
    "# Create a data loader for the test set\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False  # No need to shuffle the test data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0f87bc-cfa0-4d45-9ed3-eddff267ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135060a6-6f2f-4cee-a32e-3881d985b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden1 = 64\n",
    "hidden2 = 64\n",
    "num_classes = 10\n",
    "\n",
    "class SimpleFCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFCModel, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc1 = nn.Linear(input_size, hidden1)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2) # Second hidden layer\n",
    "        self.fc3 = nn.Linear(hidden2, num_classes) # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559a5697-2f00-4a5e-8f5f-119ea87351d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleFCModel()\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779b5632-3ac5-41da-a155-63f7c4d343a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.reshape(images, (batch_size, input_size))\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb5c432-a6e6-4668-87e9-98cbd7799974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = torch.reshape(images, (batch_size, input_size))\n",
    "        out = model(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(\"accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648a989-431b-418d-9dbf-118ca52aa7e1",
   "metadata": {},
   "source": [
    "# PART 2\n",
    "\n",
    "This part is about creating a quantized version of the model and adapting it to finn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab5b0a0-9cfd-41c7-99cc-3c2d3365cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLinear\n",
    "from brevitas.nn import QuantReLU\n",
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "brevitas_input_size = 28 * 28\n",
    "brevitas_hidden1 = 64\n",
    "brevitas_hidden2 = 64\n",
    "brevitas_num_classes = 10\n",
    "weight_bit_width = 4\n",
    "act_bit_width = 4\n",
    "dropout_prob = 0.5\n",
    "\n",
    "#is this model fully quantized or only the wieghts, i shall dig to find out once done !\n",
    "brevitas_model = nn.Sequential(\n",
    "    QuantLinear(brevitas_input_size, brevitas_hidden1, bias=True, weight_bit_width=weight_bit_width),\n",
    "    nn.BatchNorm1d(brevitas_hidden1),\n",
    "    nn.Dropout(0.5),\n",
    "    QuantReLU(bit_width=act_bit_width),\n",
    "    QuantLinear(brevitas_hidden1, brevitas_hidden2, bias=True, weight_bit_width=weight_bit_width),\n",
    "    nn.BatchNorm1d(brevitas_hidden2),\n",
    "    nn.Dropout(0.5),\n",
    "    QuantReLU(bit_width=act_bit_width),\n",
    "    QuantLinear(brevitas_hidden2, brevitas_num_classes, bias=True, weight_bit_width=weight_bit_width),\n",
    "    QuantReLU(bit_width=act_bit_width)\n",
    ")\n",
    "\n",
    "# uncomment to check the network object\n",
    "#brevitas_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289eec74",
   "metadata": {},
   "source": [
    "### The input data has to be quantized.\n",
    "\n",
    "Normaly in brevistas, we can use the ```QuantIdentity()``` layer for this but unfortunatly, it does not convert to hardware (yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "670acb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a custom quantization function\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    \n",
    "    return q_x\n",
    "\n",
    "# Define the quantized transform\n",
    "transform_quantized = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Lambda(lambda x: quantize_tensor(x))  # Apply quantization\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,  # Load the training set\n",
    "    download=True,  # Download the dataset if it doesn't exist\n",
    "    transform=transform_quantized  # Apply the defined transformations\n",
    ");\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform_quantized\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset_qnt, 100)\n",
    "test_loader = DataLoader(test_dataset_qnt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c819d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset_qnt[10]\n",
    "image = np.array(image).squeeze()\n",
    "print(\"Min : \", np.min(image), \" /// Max : \", np.max(image))\n",
    "print(image.dtype)\n",
    "# plot the sample\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236d85fe-986a-44f4-84d1-fce1d5591f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(brevitas_model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "num_epochs = 5\n",
    "brevitas_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = brevitas_model(images.float()) # This just make the value a float ie 255 becomes 255,0 and not 1\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d239cf-edb2-4faa-a502-d05861a156fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "\n",
    "brevitas_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = brevitas_model(images.float())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(\"accuracy =\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3843919-1e83-4c50-accd-3e9292cecc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets have a quick look at the weights too\n",
    "print(brevitas_model[0].quant_weight())\n",
    "#internally, weoght are stored as float 32, here nare ways to visualize actual quantized weights :\n",
    "print(brevitas_model[0].quant_weight().int())\n",
    "print(brevitas_model[0].quant_weight().int().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8247848d-6308-4524-984c-e15e9d1e71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model wrapper to :\n",
    "# - make sure the model can rack in bipolar data, we just saw so no need for that\n",
    "# - add a binary quantizer on the output whith bipolar behavior\n",
    "# note to myself, may have to rework that output quantizer (or just do no pre/post, also works fine\n",
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "\n",
    "class ModelForExport(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(ModelForExport, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out= self.pretrained(x)\n",
    "        return out\n",
    "\n",
    "model_for_export = ModelForExport(brevitas_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "058a27d4-d48a-41b4-834e-75253bb230ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "\n",
    "model_for_export.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = model_for_export(images.float())\n",
    "        # print(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(\"accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d9899-000b-4343-ac81-2c425b571519",
   "metadata": {},
   "source": [
    "# PART 3\n",
    "\n",
    "Exporting the model and visualizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11f27a3b-88c6-483e-be82-85d1ee53129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "filename = root_dir + \"/LAB_1.onnx\"\n",
    "filename_clean = root_dir + \"/LAB1_clean.onnx\"\n",
    "\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min = 0\n",
    "    max = 2**num_bits - 1\n",
    "    \n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max\n",
    "    zero_point = np.clip((-beta/scale),0,max).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(np.round(arr / scale + zero_point), min, max).astype(np.float32)\n",
    "    \n",
    "    return quantized_arr\n",
    "\n",
    "#Crete a tensor ressembling the input tensor we saw earlier\n",
    "input_a = np.random.rand(1,28*28).astype(np.float32)\n",
    "input_a = asymmetric_quantize(input_a)\n",
    "print(np.max(input_a[0]))\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    model_for_export, export_path=filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(filename, out_file=filename_clean)\n",
    "\n",
    "# ModelWrapper\n",
    "model = ModelWrapper(filename_clean)\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(root_dir + \"/ready_finn.onnx\")\n",
    "\n",
    "print(\"Model saved to \" + root_dir + \"/ready_finn.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d6f0250-5b1c-4276-9293-c29a1b112782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(root_dir + \"/ready_finn.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09bf3d-0802-4b3f-b686-fd649f5f6687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71a8839-7c58-4862-a5f0-0f7e97085918",
   "metadata": {},
   "source": [
    "# THIS NOTEBOOK IS WRONG, NOT IN THE COURSE BUT SERVES AS NOTES FOR QUANTIZE TRANSFORMATIONS\n",
    "\n",
    "After creating the docker environement,\n",
    "\n",
    "The goal of this lab is to create a MNIST Fasion model in pytorch and experiment with the different parameters\n",
    "\n",
    "Then, we will do the same model but fully quantized and start adapting it for FINN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09c1a7-e82e-4d14-bced-29aaf340638c",
   "metadata": {},
   "source": [
    "## Base model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e9dec1-299c-45ec-bc1a-41d4825fea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9651dd1-25d9-4c48-8e02-08c28d29fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    #transforms.Normalize((0.5,), (0.5,))  # Normalize the tensor with mean and std\n",
    "]);\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,  # Load the training set\n",
    "    download=True,  # Download the dataset if it doesn't exist\n",
    "    transform=transform  # Apply the defined transformations\n",
    ");\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb48f47-1369-4f87-8c9b-c0e29a8de5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min :  0.0  /// Max :  0.78039217\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset[5]\n",
    "image = np.array(image).squeeze()\n",
    "print(\"Min : \", np.min(image[0]), \" /// Max : \", np.max(image[0]))\n",
    "# plot the sample\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdce0f7-c128-48f5-956e-7174818139d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# Create a data loader for the training set\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,  # Number of samples per batch\n",
    "    shuffle=True  # Shuffle the data\n",
    ")\n",
    "\n",
    "# Create a data loader for the test set\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False  # No need to shuffle the test data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0f87bc-cfa0-4d45-9ed3-eddff267ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135060a6-6f2f-4cee-a32e-3881d985b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "hidden1 = 64\n",
    "hidden2 = 64\n",
    "num_classes = 10\n",
    "\n",
    "class SimpleFCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFCModel, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc1 = nn.Linear(input_size, hidden1)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2) # Second hidden layer\n",
    "        self.fc3 = nn.Linear(hidden2, num_classes) # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559a5697-2f00-4a5e-8f5f-119ea87351d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleFCModel(\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleFCModel()\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779b5632-3ac5-41da-a155-63f7c4d343a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3506\n",
      "Epoch [2/5], Loss: 0.3629\n",
      "Epoch [3/5], Loss: 0.4885\n",
      "Epoch [4/5], Loss: 0.4001\n",
      "Epoch [5/5], Loss: 0.2238\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.reshape(images, (batch_size, input_size))\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb5c432-a6e6-4668-87e9-98cbd7799974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 86.38\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = torch.reshape(images, (batch_size, input_size))\n",
    "        out = model(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(\"accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648a989-431b-418d-9dbf-118ca52aa7e1",
   "metadata": {},
   "source": [
    "# PART 2\n",
    "\n",
    "This part is about creating a quantized version of the model and adapting it to finn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab5b0a0-9cfd-41c7-99cc-3c2d3365cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from brevitas.nn import QuantLinear\n",
    "from brevitas.nn import QuantReLU\n",
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "brevitas_input_size = 28 * 28\n",
    "brevitas_hidden1 = 64\n",
    "brevitas_hidden2 = 64\n",
    "brevitas_num_classes = 10\n",
    "weight_bit_width = 4\n",
    "act_bit_width = 4\n",
    "dropout_prob = 0.5\n",
    "\n",
    "#is this model fully quantized or only the wieghts, i shall dig to find out once done !\n",
    "brevitas_model = nn.Sequential(\n",
    "    QuantLinear(brevitas_input_size, brevitas_hidden1, bias=True, weight_bit_width=weight_bit_width),\n",
    "    nn.BatchNorm1d(brevitas_hidden1),\n",
    "    nn.Dropout(0.5),\n",
    "    QuantReLU(bit_width=act_bit_width),\n",
    "    QuantLinear(brevitas_hidden1, brevitas_hidden2, bias=True, weight_bit_width=weight_bit_width),\n",
    "    nn.BatchNorm1d(brevitas_hidden2),\n",
    "    nn.Dropout(0.5),\n",
    "    QuantReLU(bit_width=act_bit_width),\n",
    "    QuantLinear(brevitas_hidden2, brevitas_num_classes, bias=True, weight_bit_width=weight_bit_width),\n",
    "    QuantReLU(bit_width=act_bit_width)\n",
    ")\n",
    "\n",
    "# uncomment to check the network object\n",
    "#brevitas_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289eec74",
   "metadata": {},
   "source": [
    "### The input data has to be quantized.\n",
    "\n",
    "Normaly in brevistas, we can use the ```QuantIdentity()``` layer for this but unfortunatly, it does not convert to hardware (yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "670acb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a custom quantization function\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    \n",
    "    return q_x.byte()\n",
    "\n",
    "# Define the quantized transform\n",
    "transform_quantized = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),  # Normalize the tensor with mean and std\n",
    "    transforms.Lambda(lambda x: quantize_tensor(x))  # Apply quantization\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,  # Load the training set\n",
    "    download=True,  # Download the dataset if it doesn't exist\n",
    "    transform=transform_quantized  # Apply the defined transformations\n",
    ");\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset_qnt = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,  # Load the test set\n",
    "    download=True,\n",
    "    transform=transform_quantized\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, 100)\n",
    "test_loader = DataLoader(test_dataset, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67c819d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min :  0  /// Max :  255\n",
      "uint8\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset[10]\n",
    "image = np.array(image).squeeze()\n",
    "print(\"Min : \", np.min(image), \" /// Max : \", np.max(image))\n",
    "print(image.dtype)\n",
    "# plot the sample\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "236d85fe-986a-44f4-84d1-fce1d5591f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6125\n",
      "Epoch [2/5], Loss: 0.4759\n",
      "Epoch [3/5], Loss: 0.4846\n",
      "Epoch [4/5], Loss: 0.5336\n",
      "Epoch [5/5], Loss: 0.5157\n"
     ]
    }
   ],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(brevitas_model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "num_epochs = 5\n",
    "brevitas_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = brevitas_model(images.float()) # This just make the value a float ie 255 becomes 255,0 and not 1\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4d239cf-edb2-4faa-a502-d05861a156fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 84.39 %\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "brevitas_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = brevitas_model(images.float())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(\"accuracy =\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3843919-1e83-4c50-accd-3e9292cecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantTensor(value=tensor([[ 0.0000,  0.0356,  0.1425,  ..., -0.0356, -0.0356, -0.0356],\n",
      "        [ 0.0356, -0.0356, -0.0356,  ...,  0.0356, -0.0712,  0.0712],\n",
      "        [-0.0356,  0.0000, -0.0356,  ...,  0.0712,  0.2137,  0.1068],\n",
      "        ...,\n",
      "        [-0.0712, -0.0356, -0.1068,  ...,  0.0356, -0.0712, -0.1068],\n",
      "        [-0.0356,  0.0356, -0.0356,  ..., -0.1068, -0.1781, -0.1781],\n",
      "        [ 0.0712,  0.0356,  0.0712,  ...,  0.0000, -0.0356, -0.1781]],\n",
      "       grad_fn=<MulBackward0>), scale=tensor(0.0356, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(4.), signed_t=tensor(True), training_t=tensor(False))\n",
      "tensor([[ 0,  1,  4,  ..., -1, -1, -1],\n",
      "        [ 1, -1, -1,  ...,  1, -2,  2],\n",
      "        [-1,  0, -1,  ...,  2,  6,  3],\n",
      "        ...,\n",
      "        [-2, -1, -3,  ...,  1, -2, -3],\n",
      "        [-1,  1, -1,  ..., -3, -5, -5],\n",
      "        [ 2,  1,  2,  ...,  0, -1, -5]], dtype=torch.int8)\n",
      "torch.int8\n"
     ]
    }
   ],
   "source": [
    "#lets have a quick look at the weights too\n",
    "print(brevitas_model[0].quant_weight())\n",
    "#internally, weoght are stored as float 32, here nare ways to visualize actual quantized weights :\n",
    "print(brevitas_model[0].quant_weight().int())\n",
    "print(brevitas_model[0].quant_weight().int().dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8247848d-6308-4524-984c-e15e9d1e71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model wrapper to :\n",
    "# - make sure the model can rack in bipolar data, we just saw so no need for that\n",
    "# - add a binary quantizer on the output whith bipolar behavior\n",
    "# note to myself, may have to rework that output quantizer (or just do no pre/post, also works fine\n",
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "\n",
    "class ModelForExport(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(ModelForExport, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out= self.pretrained(x)\n",
    "        return out\n",
    "\n",
    "model_for_export = ModelForExport(brevitas_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "058a27d4-d48a-41b4-834e-75253bb230ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 84.39\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "model_for_export.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "loss_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        images = torch.reshape(images, (batch_size, 28*28))\n",
    "        out = model_for_export(images.float())\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(\"accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d9899-000b-4343-ac81-2c425b571519",
   "metadata": {},
   "source": [
    "# PART 3\n",
    "\n",
    "Exporting the model and visualizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "11f27a3b-88c6-483e-be82-85d1ee53129b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n",
      "Model saved to /tmp/finn_dev_rootmin/ready_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rootmin/Documents/freelance/mission2/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "filename = \"/tmp/finn_dev_rootmin/LAB_1.onnx\"\n",
    "filename_clean = \"/tmp/finn_dev_rootmin/LAB1_clean.onnx\"\n",
    "\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min = 0\n",
    "    max = 2**num_bits - 1\n",
    "    \n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max\n",
    "    zero_point = np.clip((-beta/scale),0,max).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(np.round(arr / scale + zero_point), min, max).astype(np.float32)\n",
    "    \n",
    "    return quantized_arr\n",
    "\n",
    "#Crete a tensor ressembling the input tensor we saw earlier\n",
    "input_a = np.random.rand(1,28*28).astype(np.float32)\n",
    "input_a = asymmetric_quantize(input_a)\n",
    "print(np.max(input_a[0]))\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    model_for_export, export_path=filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(filename, out_file=filename_clean)\n",
    "\n",
    "# ModelWrapper\n",
    "model = ModelWrapper(filename_clean)\n",
    "# Setting the input datatype explicitly because it doesn't get derived from the export function\n",
    "model.set_tensor_datatype(model.graph.input[0].name, DataType[\"UINT8\"])\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(\"/tmp/finn_dev_rootmin/ready_finn.onnx\")\n",
    "\n",
    "print(\"Model saved to /tmp/finn_dev_rootmin/ready_finn.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d6f0250-5b1c-4276-9293-c29a1b112782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_rootmin/ready_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x72d4f7652740>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(\"/tmp/finn_dev_rootmin/ready_finn.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7dfbd8-95ff-4b2b-8a89-9656d1436222",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(filename_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589a8d3-f069-472c-85f3-ab2480e09fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "filename_final = \"/tmp/finn_dev_rootmin/LAB1_finnonnx.onnx\"\n",
    "\n",
    "# Create a model wrapper, convert to FINN\n",
    "model = ModelWrapper(filename_clean)\n",
    "# this is because we saw the input range from -1 to 1 but may have to be reworked\n",
    "model.set_tensor_datatype(model.graph.input[0].name, DataType[\"BIPOLAR\"])\n",
    "model = model.transform(ConvertQONNXtoFINN())\n",
    "model.save(filename_final)\n",
    "\n",
    "print(\"Model saved to %s\" % filename_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff071747-aa8c-4cac-899a-b7d3bbd92441",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(filename_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09bf3d-0802-4b3f-b686-fd649f5f6687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

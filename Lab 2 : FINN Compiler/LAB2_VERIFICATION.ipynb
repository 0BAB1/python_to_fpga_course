{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL VERIFICATION\n",
    "\n",
    "When creating the model from scratch and manipulating the data in different ways to adapt it to the use case, verifying the model at each step turns out to be important.\n",
    "\n",
    "During LAB2 notebook, we already verified the FINN-ONNX model, which indicates that we did the job right on our side, but what if something goes wrong in the way we apply further transformations ?\n",
    "\n",
    "To make sure we are ready for FPGA inference, verification is a very important step to avoid hours of useless hardware dubugging.\n",
    "\n",
    "Verifications convered by this notebook :\n",
    "\n",
    "- HLS layers verification using C++\n",
    "- RTL output verification using PyVerilator\n",
    "\n",
    "This notebook was based on [this example](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/bnn-pynq/tfc_end2end_verification.ipynb) from FINN tutorials.\n",
    "\n",
    "As you will see, verification will be fairly easy as FINN provides a very user-friendly API for these tools.\n",
    "\n",
    "## Workflow : manual transformations\n",
    "\n",
    "In this notebook, we will use [fpgadataflow transformations](https://finn.readthedocs.io/en/latest/source_code/finn.transformation.fpgadataflow.html) manualy. These usually are done automatically when building in FINN notebooks, depending on what output you ask for. Because we want control in these simulation examples, we wil use transformations \"manualy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/tmp/finn_dev_rootmin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C++ Simulation\n",
    "\n",
    "First, execute LAB2, we will grab the models from the common ```/tmp/finn_dev_yourusername/``` output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the \"golden reference\" for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "import qonnx.core.onnx_exec as oxe\n",
    "\n",
    "input_tensor = input_a = np.random.uniform(low=0, high=255, size=(28*28)).astype(np.uint8).astype(np.float32)\n",
    "input_dict = {\"global_in\": input_tensor.reshape(1,28*28)}\n",
    "golden_model = ModelWrapper(root_dir + \"/full_preproc.onnx\")\n",
    "output_dict = oxe.execute_onnx(golden_model, input_dict)\n",
    "golden_output = output_dict[list(output_dict.keys())[0]]\n",
    "\n",
    "print(golden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate the different source code : ```PrepareCppSim``` and executables : ```CompileCppSim```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_cppsim import PrepareCppSim\n",
    "from finn.transformation.fpgadataflow.compile_cppsim import CompileCppSim\n",
    "from qonnx.transformation.general import GiveUniqueNodeNames\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "model_cppsim = ModelWrapper(root_dir + \"/to_hw_conv.onnx\")\n",
    "model_cppsim = model_cppsim.transform(GiveUniqueNodeNames())\n",
    "model_cppsim = model_cppsim.transform(PrepareCppSim())\n",
    "model_cppsim = model_cppsim.transform(CompileCppSim())\n",
    "\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "\n",
    "model_cppsim.save(root_dir + \"/cppsim.onnx\")\n",
    "showInNetron(root_dir + \"/cppsim.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph manipulation reminder : [cutomOp Docs](https://finn.readthedocs.io/en/latest/source_code/finn.custom_op.html#module-qonnx.custom_op.registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the generated files\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/cppsim.onnx\")\n",
    "\n",
    "fc0 = model.graph.node[0]\n",
    "fc0w = getCustomOp(fc0)\n",
    "cpp_code_dir = fc0w.get_nodeattr(\"code_gen_dir_cppsim\")\n",
    "\n",
    "!ls {cpp_code_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.set_exec_mode import SetExecMode\n",
    "\n",
    "model_cppsim = model_cppsim.transform(SetExecMode(\"cppsim\"))\n",
    "model_cppsim.save(root_dir + \"/cppsim_exec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx.numpy_helper as nph\n",
    "import qonnx.core.onnx_exec as oxe\n",
    "\n",
    "input_dict = {\"global_in\": input_tensor.reshape(1,28*28)}\n",
    "\n",
    "parent_model = ModelWrapper(root_dir + \"/df_part.onnx\")\n",
    "sdp_node = parent_model.graph.node[0]\n",
    "child_model = root_dir + \"/cppsim_exec.onnx\"\n",
    "getCustomOp(sdp_node).set_nodeattr(\"model\", child_model)\n",
    "output_dict = oxe.execute_onnx(parent_model, input_dict)\n",
    "output_cppsim = output_dict[list(output_dict.keys())[0]]\n",
    "\n",
    "try:\n",
    "    print(golden_output[0], output_cppsim[0])\n",
    "    assert np.isclose(output_cppsim[0], np.where(golden_output[0]==np.amax(golden_output[0])), atol=1e-3).all()\n",
    "    print(\"Predictions are the same!\")\n",
    "except AssertionError:\n",
    "    assert False, \"The results are not the same!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great results are the same ! Note that this very small exmaple was done as an example and compares simple top label output. You can use the exmaple in a loop to check for hundreds of random sample et even setup a dataloader and testing loop for verification like we did like in LAB2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyVerilator RTL Verification / Emulation\n",
    "\n",
    "Once the RTL has been generated, we can also emulate it to compare with the golden result. They are multpile ways to go about this simulation, we will go for a \"node by node\" method.\n",
    "\n",
    "manual worflow comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_rtlsim import PrepareRTLSim\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.transformation.general import GiveUniqueNodeNames\n",
    "\n",
    "\n",
    "test_fpga_part = \"xc7z020clg400-1\"\n",
    "target_clk_ns = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n",
    "\n",
    "model_rtlsim = ModelWrapper(root_dir + \"/post_synth.onnx\")\n",
    "showInNetron(root_dir + \"/post_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.set_exec_mode import SetExecMode\n",
    "\n",
    "model_rtlsim = model_rtlsim.transform(SetExecMode(\"rtlsim\"))\n",
    "model_rtlsim = model_rtlsim.transform(PrepareRTLSim())\n",
    "model_rtlsim.save(root_dir + \"/rtlsim.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare parent model from dataflow partition parent model\n",
    "parent_rtsim = ModelWrapper(root_dir + \"/df_part.onnx\")\n",
    "# reference child rtl model to the streming dataflow node\n",
    "sdp_node = getCustomOp(parent_model.graph.node[0])\n",
    "sdp_node.set_nodeattr(\"model\", root_dir + \"/rtlsim.onnx\")\n",
    "\n",
    "# set the exec mode for when we'll use oxe runtime, just like with C++ simulation\n",
    "parent_rtsim = parent_rtsim.transform(SetExecMode(\"rtlsim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare input data and run oxe runtime inference in RTL SIM mode\n",
    "input_dict = {\"global_in\": input_tensor.reshape(1,28*28)}\n",
    "output_dict = oxe.execute_onnx(parent_rtsim, input_dict)\n",
    "output_rtlsim = output_dict[list(output_dict.keys())[0]]\n",
    "print(golden_output, output_rtlsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert np.isclose(output_rtlsim, np.where(golden_output[0]==np.amax(golden_output[0])), atol=1e-3).all()\n",
    "    print(\"Predictions are the same!\")\n",
    "except AssertionError:\n",
    "    assert False, \"The results are not the same!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization for Neural Networks\n",
    "\n",
    "After the small asymetric quantization example, In this notebook, we will see how to quantize a Neural Network (NN).\n",
    "\n",
    "## Post Training Quantization (PTQ)\n",
    "\n",
    "The PQT will involve training a regular model and then quantizing it.\n",
    "\n",
    "To do so, we will use observer to determine alpha, beta, scale and zero factors, whilst simply running inference. Just like we did in the f32 to int8 vector quantization example.\n",
    "\n",
    "This will be done using pytorch.\n",
    "\n",
    "## Quantization Aware Training (QAT)\n",
    "\n",
    "For this, you will have to wait until the the next lecture, where we will use Brevitas, a superset of pytorch, to do QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE BEFORE GOING FURTHER\n",
    "\n",
    "At this point in the lecture, student were not familiarized with the PyTorch so we will not enter in the details here. The goal here is to get an idea on how we can quantize a NN after training and the effects of such a thing on the model precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE DATA\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE MODEL\n",
    "# This example will be more elaborated in the second lecture, along side a full QAT example in Brevitas\n",
    "# it is okay not to understand everythong herre at this point\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECLARE THE MODEL AND OPTIMIZATION PARAMETERS\n",
    "import torch.optim as optim\n",
    "\n",
    "model = SimpleClassifier()\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0129\n",
      "Epoch [2/5], Loss: 0.0082\n",
      "Epoch [3/5], Loss: 0.0080\n",
      "Epoch [4/5], Loss: 0.3438\n",
      "Epoch [5/5], Loss: 0.0090\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# TRAIN THE MODEL\n",
    "for epoch in range(5):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Flatten the image\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/5], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.28%\n"
     ]
    }
   ],
   "source": [
    "# Testing loop\n",
    "import torch\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # Flatten the image\n",
    "        data = data.reshape(-1, 28*28)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW LET'S ANALYSE !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the model before PTQ :  408.789 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# GET MODEL SIZE\n",
    "def get_size(model):\n",
    "    torch.save(model.state_dict(), \"model_before_PTQ.p\")\n",
    "    size = os.path.getsize(\"model_before_PTQ.p\")/1e3\n",
    "    os.remove(\"model_before_PTQ.p\")\n",
    "    return(size)\n",
    "\n",
    "print(\"size of the model before PTQ : \", get_size(model), \"KB\")\n",
    "\n",
    "# GET MODEL DATA (note : we'll see this is a different API call for QAT using Brevitas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "\n",
    "This notebook serves as a basic comprehensive introductoin to quantization by manipulating np arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets create a FP32 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "my_fp_data = np.random.rand(50).astype(np.float32)\n",
    "my_fp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now create a quantization function and quantize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min = 0\n",
    "    max = 2**num_bits - 1\n",
    "    \n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max\n",
    "    zero_point = np.clip((-beta/scale),0,max).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(np.round(arr / scale + zero_point), min, max).astype(np.uint8)\n",
    "    \n",
    "    return quantized_arr, scale, zero_point\n",
    "\n",
    "def asymmetric_dequantize(quantized_arr, scale, zero_point):\n",
    "    return (quantized_arr.astype(np.float32) - zero_point) * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_int_data, scale, zero = asymmetric_quantize(my_fp_data)\n",
    "my_int_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's recover our inital data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequant = asymmetric_dequantize(my_int_data, scale, zero)\n",
    "dequant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see the values are close but not the same...\n",
    "\n",
    "This error is induced by the lowering of bits resolutions.\n",
    "It may look bad but it's not !\n",
    "It allows for :\n",
    "- Lighter models (int8 << fp32 in size)\n",
    "- Faster operations within the model\n",
    "\n",
    "Of course this error has an effect, let's see those effects on NN inference\n",
    "and how to quantize a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further ...\n",
    "\n",
    "If quantization sparked your curiosity, I recomend you watch this 1hour video that will go over in details what quantization is for NN : [Video](https://www.youtube.com/watch?v=0VdNflU08yA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
